{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qSmCvZ5GtEut"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnI7_A9LtezZ"},"outputs":[],"source":["# Constants\n","TRAIN_SOURCE_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/train_source.txt\"\n","TRAIN_TARGET_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/train_target.txt\"\n","DEV_SOURCE_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/dev_source.txt\"\n","DEV_TARGET_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/dev_target.txt\"\n","GEN_SOURCE_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/gen_source.txt\"\n","GEN_TARGET_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/gen_target.txt\"\n","SOURCE_VOCAB_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/source_vocab.txt\"\n","TARGET_VOCAB_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/target_vocab.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmtN90J2uMSb"},"outputs":[],"source":["BATCH_SIZE = 32\n","LEARNING_RATE = 0.001\n","NUM_EPOCHS = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tSuR3fEuSst"},"outputs":[],"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIr7tBa3uXOQ"},"outputs":[],"source":["class Seq2SeqDataset(Dataset):\n","    def __init__(self, source_file, target_file, source_vocab_file, target_vocab_file):\n","        self.source_data = self.load_data(source_file)\n","        self.target_data = self.load_data(target_file)\n","        self.source_vocab = self.load_vocab(source_vocab_file)\n","        self.target_vocab = self.load_vocab(target_vocab_file)\n","        self.max_seq_length = 100  # Example value, adjust based on your data\n","\n","        # Ensure '<pad>' token exists in target_vocab, if not, add it\n","        if '<pad>' not in self.target_vocab:\n","            self.target_vocab['<pad>'] = len(self.target_vocab)\n","\n","    def __len__(self):\n","        return len(self.source_data)\n","\n","    def __getitem__(self, idx):\n","        source_seq = self.process_sequence(self.source_data[idx], self.source_vocab)\n","        target_seq = self.process_sequence(self.target_data[idx], self.target_vocab)\n","        return source_seq, target_seq\n","\n","    def load_data(self, file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            data = [line.strip().split() for line in file.readlines()]\n","        return data\n","\n","    def load_vocab(self, vocab_file):\n","        with open(vocab_file, 'r', encoding='utf-8') as file:\n","            vocab = {token.strip(): idx for idx, token in enumerate(file.readlines())}\n","        return vocab\n","\n","    def process_sequence(self, sequence, vocab):\n","        # Convert tokens to indices; pad or truncate to max_seq_length\n","        indexed_seq = [vocab[token] if token in vocab else vocab['<unk>'] for token in sequence]\n","        indexed_seq = indexed_seq[:self.max_seq_length] + [vocab['<pad>']] * (self.max_seq_length - len(indexed_seq))\n","        return torch.tensor(indexed_seq, dtype=torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46JJL2rtue4l"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JBwZdxvumlx"},"outputs":[],"source":["class UniversalTransformer(nn.Module):\n","    def __init__(self, input_vocab_size, output_vocab_size, d_model=512, num_heads=4, num_layers=2):\n","        super(UniversalTransformer, self).__init__()\n","        self.embedding = nn.Embedding(input_vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model)\n","\n","        # Transformer Encoder Layers\n","        encoder_layers = nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward=512)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n","\n","        self.fc = nn.Linear(d_model, output_vocab_size)\n","\n","    def forward(self, src):\n","        src = self.embedding(src)\n","        src = self.positional_encoding(src)\n","        output = self.transformer_encoder(src)\n","        output = self.fc(output)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPPUagXvusCv"},"outputs":[],"source":["def train_epoch(model, optimizer, criterion, train_loader, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","\n","    for src, tgt in train_loader:\n","        src, tgt = src.to(device), tgt.to(device)\n","        optimizer.zero_grad()\n","\n","        output = model(src)\n","        loss = criterion(output.view(-1, output.shape[-1]), tgt.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        total_correct += (output.argmax(-1) == tgt).sum().item()\n","\n","    return total_loss / len(train_loader.dataset), total_correct / len(train_loader.dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWr1ZjdJuxos"},"outputs":[],"source":["def evaluate(model, criterion, data_loader, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","\n","    with torch.no_grad():\n","        for src, tgt in data_loader:\n","            src, tgt = src.to(device), tgt.to(device)\n","            output = model(src)\n","            loss = criterion(output.view(-1, output.shape[-1]), tgt.view(-1))\n","            total_loss += loss.item()\n","            total_correct += (output.argmax(-1) == tgt).sum().item()\n","\n","    return total_loss / len(data_loader.dataset), total_correct / len(data_loader.dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mp5v_j1bu2VH"},"outputs":[],"source":["# Load datasets\n","train_dataset = Seq2SeqDataset(TRAIN_SOURCE_FILE, TRAIN_TARGET_FILE, SOURCE_VOCAB_FILE, TARGET_VOCAB_FILE)\n","dev_dataset = Seq2SeqDataset(DEV_SOURCE_FILE, DEV_TARGET_FILE, SOURCE_VOCAB_FILE, TARGET_VOCAB_FILE)\n","gen_dataset = Seq2SeqDataset(GEN_SOURCE_FILE, GEN_TARGET_FILE, SOURCE_VOCAB_FILE, TARGET_VOCAB_FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEsU2-1Mu69W"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","gen_loader = DataLoader(gen_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0dWWcuHvAsQ"},"outputs":[],"source":["source_vocab_size = len(train_dataset.source_vocab)\n","target_vocab_size = len(train_dataset.target_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3261,"status":"ok","timestamp":1719224016211,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"},"user_tz":-330},"id":"rW0Z7OZLvFjw","outputId":"8947729f-39b9-4099-863c-d35ce89b88ad"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}],"source":["model = UniversalTransformer(source_vocab_size, target_vocab_size).to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.target_vocab['<pad>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvRfKfM4vK7q","outputId":"ad2a7650-1d53-4ade-faa0-e87447795d15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1:\n","  Train Loss: 0.0956 | Train Acc: 8.4623\n","  Dev Loss: 0.0946 | Dev Acc: 8.8867\n","Epoch 2:\n","  Train Loss: 0.0943 | Train Acc: 8.7081\n","  Dev Loss: 0.0939 | Dev Acc: 9.1677\n","Epoch 3:\n","  Train Loss: 0.0946 | Train Acc: 8.6733\n","  Dev Loss: 0.0945 | Dev Acc: 8.9097\n","Epoch 4:\n","  Train Loss: 0.0947 | Train Acc: 8.6134\n","  Dev Loss: 0.0940 | Dev Acc: 8.8867\n","Epoch 5:\n","  Train Loss: 0.0941 | Train Acc: 8.7987\n","  Dev Loss: 0.0942 | Dev Acc: 9.0857\n","Epoch 6:\n","  Train Loss: 0.0941 | Train Acc: 8.8043\n","  Dev Loss: 0.0940 | Dev Acc: 8.8233\n"]}],"source":["# Training loop\n","for epoch in range(NUM_EPOCHS):\n","    train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader, DEVICE)\n","    dev_loss, dev_acc = evaluate(model, criterion, dev_loader, DEVICE)\n","\n","    print(f\"Epoch {epoch + 1}:\")\n","    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n","    print(f\"  Dev Loss: {dev_loss:.4f} | Dev Acc: {dev_acc:.4f}\")\n","\n","# Evaluate on generated data\n","gen_loss, gen_acc = evaluate(model, criterion, gen_loader, DEVICE)\n","print(f\"Generated Data:\")\n","print(f\"  Gen Loss: {gen_loss:.4f} | Gen Acc: {gen_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":173901,"status":"ok","timestamp":1719299742938,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"},"user_tz":-330},"id":"JPGjXH3zJvHX","outputId":"ca7174cd-f002-4961-da96-6d0598abb6e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20 | Time: 8.76s\n","Train Loss: 0.0497, Train Accuracy: 0.1772\n","Epoch 2/20 | Time: 9.00s\n","Train Loss: 0.0491, Train Accuracy: 0.1805\n","Epoch 3/20 | Time: 9.02s\n","Train Loss: 0.0491, Train Accuracy: 0.1801\n","Epoch 4/20 | Time: 8.94s\n","Train Loss: 0.0491, Train Accuracy: 0.1806\n","Epoch 5/20 | Time: 10.23s\n","Train Loss: 0.0491, Train Accuracy: 0.1803\n","Epoch 6/20 | Time: 7.94s\n","Train Loss: 0.0491, Train Accuracy: 0.1805\n","Epoch 7/20 | Time: 8.58s\n","Train Loss: 0.0491, Train Accuracy: 0.1805\n","Epoch 8/20 | Time: 8.14s\n","Train Loss: 0.0491, Train Accuracy: 0.1804\n","Epoch 9/20 | Time: 8.45s\n","Train Loss: 0.0491, Train Accuracy: 0.1807\n","Epoch 10/20 | Time: 8.58s\n","Train Loss: 0.0491, Train Accuracy: 0.1802\n","Epoch 11/20 | Time: 8.05s\n","Train Loss: 0.0491, Train Accuracy: 0.1803\n","Epoch 12/20 | Time: 9.10s\n","Train Loss: 0.0491, Train Accuracy: 0.1805\n","Epoch 13/20 | Time: 8.48s\n","Train Loss: 0.0491, Train Accuracy: 0.1805\n","Epoch 14/20 | Time: 8.30s\n","Train Loss: 0.0491, Train Accuracy: 0.1808\n","Epoch 15/20 | Time: 8.64s\n","Train Loss: 0.0491, Train Accuracy: 0.1803\n","Epoch 16/20 | Time: 8.07s\n","Train Loss: 0.0491, Train Accuracy: 0.1803\n","Epoch 17/20 | Time: 8.70s\n","Train Loss: 0.0491, Train Accuracy: 0.1807\n","Epoch 18/20 | Time: 8.62s\n","Train Loss: 0.0491, Train Accuracy: 0.1808\n","Epoch 19/20 | Time: 8.81s\n","Train Loss: 0.0491, Train Accuracy: 0.1803\n","Epoch 20/20 | Time: 8.65s\n","Train Loss: 0.0491, Train Accuracy: 0.1804\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import time\n","\n","# Constants\n","# Constants\n","TRAIN_SOURCE_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/train_source.txt\"\n","TRAIN_TARGET_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/train_target.txt\"\n","DEV_SOURCE_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/dev_source.txt\"\n","DEV_TARGET_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/dev_target.txt\"\n","GEN_SOURCE_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/gen_source.txt\"\n","GEN_TARGET_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/gen_target.txt\"\n","SOURCE_VOCAB_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/source_vocab.txt\"\n","TARGET_VOCAB_FILE = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/target_vocab.txt\"\n","\n","\n","BATCH_SIZE = 64  # Increase batch size for faster training\n","LEARNING_RATE = 0.001\n","NUM_EPOCHS = 20\n","MAX_SEQ_LENGTH = 100\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Dataset class\n","class Seq2SeqDataset(Dataset):\n","    def __init__(self, source_file, target_file, source_vocab_file, target_vocab_file):\n","        self.source_data = self.load_data(source_file)\n","        self.target_data = self.load_data(target_file)\n","        self.source_vocab = self.load_vocab(source_vocab_file)\n","        self.target_vocab = self.load_vocab(target_vocab_file)\n","\n","        if '<pad>' not in self.target_vocab:\n","            self.target_vocab['<pad>'] = len(self.target_vocab)\n","\n","    def __len__(self):\n","        return len(self.source_data)\n","\n","    def __getitem__(self, idx):\n","        source_seq = self.process_sequence(self.source_data[idx], self.source_vocab)\n","        target_seq = self.process_sequence(self.target_data[idx], self.target_vocab)\n","        return source_seq, target_seq\n","\n","    def load_data(self, file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            data = [line.strip().split() for line in file.readlines()]\n","        return data\n","\n","    def load_vocab(self, vocab_file):\n","        with open(vocab_file, 'r', encoding='utf-8') as file:\n","            vocab = {token.strip(): idx for idx, token in enumerate(file.readlines())}\n","        return vocab\n","\n","    def process_sequence(self, sequence, vocab):\n","        indexed_seq = [vocab[token] if token in vocab else vocab['<unk>'] for token in sequence]\n","        indexed_seq = indexed_seq[:MAX_SEQ_LENGTH] + [vocab['<pad>']] * (MAX_SEQ_LENGTH - len(indexed_seq))\n","        return torch.tensor(indexed_seq, dtype=torch.long)\n","\n","# Positional Encoding class\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","# Universal Transformer class\n","class UniversalTransformer(nn.Module):\n","    def __init__(self, input_vocab_size, output_vocab_size, d_model=512, num_heads=4, num_layers=2):\n","        super(UniversalTransformer, self).__init__()\n","        self.embedding = nn.Embedding(input_vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model)\n","\n","        encoder_layers = nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward=512, dropout=0.1)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n","\n","        self.fc = nn.Linear(d_model, output_vocab_size)\n","\n","    def forward(self, src):\n","        src = self.embedding(src)\n","        src = self.positional_encoding(src)\n","        output = self.transformer_encoder(src)\n","        output = self.fc(output)\n","        return output\n","\n","# Training function\n","def train_epoch(model, optimizer, criterion, train_loader, device, scheduler, scaler):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_count = 0\n","\n","    for src, tgt in train_loader:\n","        src, tgt = src.to(device), tgt.to(device)\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            output = model(src)\n","            loss = criterion(output.view(-1, output.shape[-1]), tgt.view(-1))\n","\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","        pred = output.argmax(-1)\n","        non_pad_elements = (tgt != train_loader.dataset.target_vocab['<pad>']).sum().item()\n","        total_correct += (pred == tgt).sum().item() - (pred[tgt == train_loader.dataset.target_vocab['<pad>']] == train_loader.dataset.target_vocab['<pad>']).sum().item()\n","        total_count += non_pad_elements\n","\n","    return total_loss / len(train_loader.dataset), total_correct / total_count\n","\n","# Load dataset and create DataLoader\n","train_dataset = Seq2SeqDataset(TRAIN_SOURCE_FILE, TRAIN_TARGET_FILE, SOURCE_VOCAB_FILE, TARGET_VOCAB_FILE)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n","\n","# Create model, optimizer, and loss function\n","model = UniversalTransformer(len(train_dataset.source_vocab), len(train_dataset.target_vocab)).to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n","criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.target_vocab['<pad>'])\n","\n","scaler = torch.cuda.amp.GradScaler()\n","\n","# Training loop\n","for epoch in range(NUM_EPOCHS):\n","    start_time = time.time()\n","    train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader, DEVICE, scheduler, scaler)\n","    elapsed_time = time.time() - start_time\n","    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} | Time: {elapsed_time:.2f}s\")\n","    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79624,"status":"ok","timestamp":1719317683502,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"},"user_tz":-330},"id":"LbL07mS5UA-y","outputId":"d1a574d3-69a9-4820-dbe3-2359691a4ccb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow<2.17,>=2.16.1 (from tensorflow-text)\n","  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.2.0)\n","Collecting h5py>=3.10.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (18.1.1)\n","Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.64.1)\n","Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading keras-3.4.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.37.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.25.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (13.7.1)\n","Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2024.6.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.1.2)\n","Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tensorflow-text\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.9.0\n","    Uninstalling h5py-3.9.0:\n","      Successfully uninstalled h5py-3.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.4.0 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-text-2.16.1\n"]}],"source":["!pip install tensorflow\n","!pip install tensorflow-text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6351,"status":"ok","timestamp":1719319912534,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"},"user_tz":-330},"id":"5KZu57zic5pL","outputId":"fdd19357-3592-4a4b-9103-d039613d8224"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.2)\n"]}],"source":["!pip install tokenizers"]},{"cell_type":"code","source":["pip install torch transformers tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlyrjljqEeLK","executionInfo":{"status":"ok","timestamp":1719397450374,"user_tz":-330,"elapsed":70561,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"}},"outputId":"74d30c47-749a-473a-be8b-c819bd429932"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","# Function to load vocabularies\n","def load_vocab(filepath):\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        vocab = f.read().splitlines()\n","    return vocab\n","\n","# Load source and target vocabularies\n","source_vocab = load_vocab(\"/content/drive/MyDrive/IIITH/COGS-main/output_path/source_vocab.txt\")\n","target_vocab = load_vocab(\"/content/drive/MyDrive/IIITH/COGS-main/output_path/target_vocab.txt\")\n","\n","# Tokenizer initialization\n","source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n","target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n","\n","# Fit tokenizer on vocabularies\n","source_tokenizer.fit_on_texts(source_vocab)\n","target_tokenizer.fit_on_texts(target_vocab)\n","\n","# Vocabulary sizes\n","source_vocab_size = len(source_tokenizer.word_index) + 1\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","# Function to load and preprocess data\n","def load_data(source_file, target_file, source_tokenizer, target_tokenizer, max_seq_length=None):\n","    # Load source and target data from files\n","    with open(source_file, 'r', encoding='utf-8') as f:\n","        source_data = f.read().splitlines()\n","    with open(target_file, 'r', encoding='utf-8') as f:\n","        target_data = f.read().splitlines()\n","\n","    # Tokenize source and target data\n","    source_sequences = source_tokenizer.texts_to_sequences(source_data)\n","    target_sequences = target_tokenizer.texts_to_sequences(target_data)\n","\n","    # Pad sequences to max_seq_length if provided, otherwise pad to maximum sequence length in the data\n","    if max_seq_length:\n","        source_sequences = tf.keras.preprocessing.sequence.pad_sequences(source_sequences, padding='post', maxlen=max_seq_length)\n","        target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, padding='post', maxlen=max_seq_length)\n","    else:\n","        source_sequences = tf.keras.preprocessing.sequence.pad_sequences(source_sequences, padding='post')\n","        target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, padding='post')\n","\n","    return source_sequences, target_sequences\n","\n","# File paths\n","train_source_file = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/train_source.txt\"\n","train_target_file = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/train_target.txt\"\n","gen_source_file = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/gen_source.txt\"\n","gen_target_file = \"/content/drive/MyDrive/IIITH/COGS-main/output_path/gen_target.txt\"\n","\n","# Load and preprocess training and generation data\n","train_source, train_target = load_data(train_source_file, train_target_file, source_tokenizer, target_tokenizer)\n","gen_source, gen_target = load_data(gen_source_file, gen_target_file, source_tokenizer, target_tokenizer)\n","\n","train_source = tf.cast(train_source, dtype=tf.float32)\n","train_target = tf.cast(train_target, dtype=tf.float32)\n","gen_source = tf.cast(gen_source, dtype=tf.float32)\n","gen_target = tf.cast(gen_target, dtype=tf.float32)\n","\n","# Display shape of loaded data\n","print(f\"Training Source shape: {train_source.shape}, Training Target shape: {train_target.shape}\")\n","print(f\"Generation Source shape: {gen_source.shape}, Generation Target shape: {gen_target.shape}\")\n","\n","# Example of how to access tokenized sequences\n","print(\"Example of tokenized source sequence:\")\n","print(train_source[0])\n","print(\"Example of tokenized target sequence:\")\n","print(train_target[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrxmZg9g2v0H","executionInfo":{"status":"ok","timestamp":1719511959695,"user_tz":-330,"elapsed":2419,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"}},"outputId":"55c173dc-28ae-4a29-93ed-651eff783aca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Source shape: (24155, 22), Training Target shape: (24155, 153)\n","Generation Source shape: (21000, 61), Generation Target shape: (21000, 480)\n","Example of tokenized source sequence:\n","tf.Tensor(\n","[  2. 521. 246. 570. 397.   2. 428. 701.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.], shape=(22,), dtype=float32)\n","Example of tokenized target sequence:\n","tf.Tensor(\n","[516.  91. 485. 569. 254. 282. 662. 634. 685. 181.  91. 485. 569.  60.\n"," 101. 485. 569. 254. 282. 662. 634. 685. 170.  91. 485. 569.  60. 101.\n"," 485. 569. 256. 282. 662. 418.  91. 485. 569. 256. 282.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.], shape=(153,), dtype=float32)\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout, Dense\n","\n","# Positional Encoding function\n","def positional_encoding(position, d_model):\n","    angle_rads = np.linspace(0, 2 * np.pi, d_model // 2)\n","    angle_rads = angle_rads.reshape(1, d_model // 2)\n","    pos = np.arange(position).reshape(position, 1)\n","    pos_encoding = pos / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n","    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n","    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n","    pos_encoding = tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)  # Ensure dtype is float32\n","    return pos_encoding\n","\n","# Universal Transformer model\n","class UniversalTransformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(UniversalTransformer, self).__init__()\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding_input = tf.cast(pe_input, dtype=tf.float32)\n","        self.pos_encoding_target = tf.cast(pe_target, dtype=tf.float32)\n","        self.encoder_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.decoder_layers = [TransformerDecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.final_layer = Dense(target_vocab_size)\n","\n","    def call(self, inp, training=True):\n","        seq_len = tf.shape(inp)[1]\n","        inp = tf.cast(self.embedding(inp), dtype=tf.float32) + self.pos_encoding_input[:, :seq_len, :]\n","\n","        for i in range(self.num_layers):\n","            inp = self.encoder_layers[i](inp, training)\n","\n","        return inp\n","\n","    def decode(self, tar, enc_output, training=True):\n","        seq_len = tf.shape(tar)[1]\n","        tar = self.embedding(tar) + self.pos_encoding_target[:, :seq_len, :]\n","\n","        for i in range(self.num_layers):\n","            tar = self.decoder_layers[i](tar, enc_output, training)\n","\n","        final_output = self.final_layer(tar)\n","        return final_output\n","\n","# Transformer Encoder Layer\n","class TransformerEncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n","        self.dropout1 = Dropout(rate)\n","        self.layer_norm1 = LayerNormalization(epsilon=1e-6)\n","        self.dense1 = Dense(dff, activation='relu')\n","        self.dense2 = Dense(d_model)\n","        self.dropout2 = Dropout(rate)\n","        self.layer_norm2 = LayerNormalization(epsilon=1e-6)\n","\n","    def call(self, x, training=True):\n","        attn_output = self.multi_head_attention(x, x, return_attention_scores=False)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layer_norm1(x + attn_output)\n","\n","        ffn_output = self.dense2(self.dense1(out1))\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layer_norm2(out1 + ffn_output)\n","\n","        return out2\n","\n","# Transformer Decoder Layer\n","class TransformerDecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerDecoderLayer, self).__init__()\n","        self.multi_head_attention1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n","        self.dropout1 = Dropout(rate)\n","        self.layer_norm1 = LayerNormalization(epsilon=1e-6)\n","        self.multi_head_attention2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n","        self.dropout2 = Dropout(rate)\n","        self.layer_norm2 = LayerNormalization(epsilon=1e-6)\n","        self.dense1 = Dense(dff, activation='relu')\n","        self.dense2 = Dense(d_model)\n","        self.dropout3 = Dropout(rate)\n","        self.layer_norm3 = LayerNormalization(epsilon=1e-6)\n","\n","    def call(self, x, enc_output, training=True):\n","        attn1 = self.multi_head_attention1(x, x, return_attention_scores=False)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layer_norm1(attn1 + x)\n","\n","        attn2 = self.multi_head_attention2(enc_output, out1, return_attention_scores=False)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layer_norm2(attn2 + out1)\n","\n","        ffn_output = self.dense2(self.dense1(out2))\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layer_norm3(out2 + ffn_output)\n","\n","        return out3\n","\n","\n","\n","# Assuming train_source, train_target, gen_source, gen_target are TensorFlow tensors\n","train_source = tf.cast(train_source, dtype=tf.int32)\n","train_target = tf.cast(train_target, dtype=tf.int32)\n","gen_source = tf.cast(gen_source, dtype=tf.int32)\n","gen_target = tf.cast(gen_target, dtype=tf.int32)\n","\n","num_layers = 2\n","d_model = 128\n","num_heads = 4\n","dff = 512\n","input_vocab_size = len(source_tokenizer.word_index) + 1\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","dropout_rate = 0.1\n","\n","# Initialize positional encodings\n","pe_input = positional_encoding(10000, d_model)\n","pe_target = positional_encoding(6000, d_model)\n","\n","# Initialize model\n","transformer = UniversalTransformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size,\n","                                   pe_input=pe_input,\n","                                   pe_target=pe_target,\n","                                   rate=dropout_rate)\n","\n","# Compile and fit model\n","transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","history = transformer.fit(train_source, train_target, epochs=10, validation_data=(gen_source, gen_target))\n","\n","# Evaluate model\n","loss, accuracy = transformer.evaluate(gen_source, gen_target)\n","print(f'Evaluation loss: {loss}, accuracy: {accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":842},"id":"-UySejaF56WK","executionInfo":{"status":"error","timestamp":1719513592924,"user_tz":-330,"elapsed":3047,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"}},"outputId":"f17ffee1-3077-45df-87e0-bdb66f11d997"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 969, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 153 and 22 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_3, Cast_5)' with input shapes: [?,153], [?,22].\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-74446e4bd54a>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Compile and fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 969, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 153 and 22 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_3, Cast_5)' with input shapes: [?,153], [?,22].\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","# Parameters\n","max_source_length = 22  # Length of the source sentences\n","max_target_length = 153  # Length of the target logical forms\n","vocab_size = 10000  # Adjust based on your vocabulary size\n","embedding_dim = 256\n","num_heads = 4\n","num_layers = 2\n","dropout_rate = 0.1\n","\n","# Load your data\n","def load_data(source_file, target_file):\n","    with open(source_file, 'r') as f:\n","        source_data = f.readlines()\n","    with open(target_file, 'r') as f:\n","        target_data = f.readlines()\n","    return source_data, target_data\n","\n","# Convert text to sequences\n","def text_to_sequences(text, vocab):\n","    sequences = []\n","    for line in text:\n","        sequences.append([vocab.get(word, vocab['<unk>']) for word in line.strip().split()])\n","    return sequences\n","\n","# Load vocab files\n","def load_vocab(vocab_file):\n","    with open(vocab_file, 'r') as f:\n","        vocab = {word.strip(): i for i, word in enumerate(f.readlines())}\n","    return vocab\n","\n","source_vocab = load_vocab('/content/drive/MyDrive/IIITH/COGS-main/output_path/source_vocab.txt')\n","target_vocab = load_vocab('/content/drive/MyDrive/IIITH/COGS-main/output_path/target_vocab.txt')\n","\n","# Load data\n","train_source, train_target = load_data('/content/drive/MyDrive/IIITH/COGS-main/output_path/train_source.txt', '/content/drive/MyDrive/IIITH/COGS-main/output_path/train_target.txt')\n","\n","# Convert text to sequences\n","train_source_sequences = text_to_sequences(train_source, source_vocab)\n","train_target_sequences = text_to_sequences(train_target, target_vocab)\n","\n","# Pad sequences\n","train_source_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_source_sequences, maxlen=max_source_length, padding='post')\n","train_target_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_target_sequences, maxlen=max_target_length, padding='post')\n","\n","# Prepare the target sequences for training the decoder\n","train_target_input = train_target_sequences[:, :-1]  # all tokens except the last one\n","train_target_output = train_target_sequences[:, 1:]  # all tokens except the first one\n","class UniversalTransformer(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, dropout_rate):\n","        super(UniversalTransformer, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.transformer_layers = [\n","            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim) for _ in range(num_layers)\n","        ]\n","        self.layer_norm = [tf.keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","        self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, x, training):\n","        x = self.embedding(x)\n","        for i in range(num_layers):\n","            attn_output = self.transformer_layers[i](x, x)\n","            attn_output = self.dropout(attn_output, training=training)\n","            x = self.layer_norm[i](x + attn_output)\n","        x = self.dense(x)\n","        return x\n","\n","# Create and compile the model\n","model = UniversalTransformer(vocab_size, embedding_dim, num_heads, num_layers, dropout_rate)\n","model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","# Prepare the data for training\n","batch_size = 64\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_source_sequences, train_target_input, train_target_output))\n","train_dataset = train_dataset.batch(batch_size)\n","\n","# Custom training loop\n","epochs = 10\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","@tf.function\n","def train_step(source_seq, target_inp, target_out):\n","    with tf.GradientTape() as tape:\n","        predictions = model(source_seq, training=True)\n","        # Match the target_out shape to predictions shape\n","        target_out = tf.reshape(target_out, [-1, tf.shape(predictions)[1], tf.shape(predictions)[2]])\n","        loss = loss_object(target_out, predictions)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    train_loss(loss)\n","    train_accuracy(target_out, predictions)\n","\n","for epoch in range(epochs):\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","\n","    for batch, (source_seq, target_inp, target_out) in enumerate(train_dataset):\n","        train_step(source_seq, target_inp, target_out)\n","\n","    print(f'Epoch {epoch + 1}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result() * 100}')\n","\n","# Save the model\n","model.save('universal_transformer.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"C4f9OOnzwoaF","executionInfo":{"status":"error","timestamp":1719568704102,"user_tz":-330,"elapsed":2172,"user":{"displayName":"Varaprasad Tarunkumar","userId":"08209076529172229742"}},"outputId":"9e1859f7-6a4c-461d-855d-c542525411f7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    File \"<ipython-input-11-8dc5a5d352f7>\", line 91, in train_step  *\n        target_out = tf.reshape(target_out, [-1, tf.shape(predictions)[1], tf.shape(predictions)[2]])\n\n    ValueError: Dimension size must be evenly divisible by 220000 but is 9728 for '{{node Reshape}} = Reshape[T=DT_INT32, Tshape=DT_INT32](target_out, Reshape/shape)' with input shapes: [64,152], [3] and with input tensors computed as partial shapes: input[1] = [?,22,10000].\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-8dc5a5d352f7>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result() * 100}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_filefy2silgo.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(source_seq, target_inp, target_out)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     \u001b[0mtarget_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-11-8dc5a5d352f7>\", line 91, in train_step  *\n        target_out = tf.reshape(target_out, [-1, tf.shape(predictions)[1], tf.shape(predictions)[2]])\n\n    ValueError: Dimension size must be evenly divisible by 220000 but is 9728 for '{{node Reshape}} = Reshape[T=DT_INT32, Tshape=DT_INT32](target_out, Reshape/shape)' with input shapes: [64,152], [3] and with input tensors computed as partial shapes: input[1] = [?,22,10000].\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1izveRLn3miQDgfp9tZIk3wHMA3pt4HLy","authorship_tag":"ABX9TyNG3KpUiSAvZwtN5p9I3IIO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}